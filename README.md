# AlgoLang-Compiler

## Overview

1. **Lexical Analyzer (`lex.py`)**: Tokenizes the input code.
2. **Syntax Analyzer (`syn.py`)**: Validates the structure of the tokenized code.
3. **Compiler (`main.py`)**: Integrates the lexical and syntax analyzers to compile the input code.

## Project Structure

```bash
src/
├──app/
   ├── main.py           # Main entry point of the compiler
   ├── lex.py            # Lexical Analyzer
   ├── syn.py            # Syntax Analyzer
   ├── automate.py       # Rules and states for lexical analyzer
   ├── input.txt         # Input code to compile 
   ├── token.txt         # Output tokens (generated by lexical analyzer)
   diagrams/
   ├── automate.drawio   # Go to draw `https://app.diagrams.net/` to run it 
   ├── automate.png      # Implementation of the Lexical Analyzer

```

### Usage

#### Running the Compiler
The compiler can now take input and output file paths via command-line arguments (its optional). Use the following command:

```bash
python main.py -i <input_file_path> -o <output_file_path>
```
#### Default Values

`<input_file_path>` = `input.txt`

`<output_file_path>` =  `token.txt`



## Files

### main.py
The main entry point for the compiler. It:
- Reads the input file (`input.txt`).
- Invokes the lexical analyzer to tokenize the code.
- Passes the tokens to the syntax analyzer for validation.


#### Key Methods
- `__init__(self, input_path, output_path)`: Initializes file paths and analyzers.
- `compile(self)`: Performs compilation by invoking the lexical and syntax analyzers.

### lex.py
Implements the **Lexical Analyzer** to tokenize the input code based on rules defined in `automate.py`.

#### Key Methods
- `read_file(self, file_path)`: Reads the input file.
- `lexical_analyzer(self, file_path, output_path, save)`: Tokenizes the input file .
- `save_tokens(self, tokens, file_path)`: Writes tokens to the output file.
- `run(self, code)`: Run eveything together.

### syn.py
Implements the **Syntax Analyzer**, which validates the structure of tokenized code based on the grammar of the language.

#### Key Methods
- `syntax_analyzer(self, file_path)`: Validates the tokens from the lexical analyzer.
- `match(self, expected_token_type, type)`: Matches current token with expected type.
- Parsing methods (e.g., `parse_ProgrammeAlgoLang`, `parse_Corps`) to validate the syntax of various constructs.

### automate.py
Defines the rules and final states for lexical analysis. Uses finite state machine logic to determine valid tokens and their classifications.

#### Key Components
- `rules`: Dictionary defining transitions for the state machine.
- `final_states`: Dictionary mapping final states to token types.


## Example Usage
Input Code (`input.txt`):

```bash
programme x1;
variable x2, y3 : entier;
debut
x4 := 5;
fin.
```

Run the compiler:

```bash
python main.py
```

Output (`token.txt`):

```bash
programme|Keyword_programme
x1|Name
;|Semicolon
variable|Keyword_variable
x2|Name
,|COMMA
y3|Name
:|COLON
entier|TypeName_entier
;|Semicolon
debut|Keyword_debut
x4|Name
:=|ASSIGNMENT
5|Number
;|Semicolon
fin|Keyword_fin
.|END
```

stdout (`command line`)

```bash
Compilation done successfully
```
